import os
    
import sys
import logging
import socket


from langchain.chains import RetrievalQA
#from langchain_openai import ChatOpenAI

# ä¼˜å…ˆä½¿ç”¨å·²å®‰è£…çš„ langchainï¼ˆ0.2.xï¼‰ä¸­çš„ç±»ï¼Œå…¼å®¹ pydantic 1.x
try:
    from langchain.chat_models import ChatOpenAI, AzureChatOpenAI
except Exception:
    # å›žé€€ï¼ˆå¦‚æžœä½ ç¡®å®žå®‰è£…äº† langchain-openai ä¸”çŽ¯å¢ƒå…è®¸ï¼‰
    from langchain_openai import ChatOpenAI, AzureChatOpenAI



sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))
from gpt_client.gpt_client.prompts.prompt_template import QA_TEMPLATE_BAICHUAN
import gpt_client.gpt_client.commons.embedding_utils as eu
from gpt_client.gpt_client.commons.utils import *

logging.basicConfig(level=logging.INFO)


class GPTAssistant:
    """ Load ChatGPT config and your custom pre-prompts. """

    def __init__(self, verbose=False) -> None:
        
        logging.info("Loading keys...")
        cfg_file = os.path.join(os.path.dirname(__file__), '../commons/config.json')
        set_global_configs(cfg_file)
        logging.info(f"Done.")

        logging.info("Initialize LLM...")
        #llm = ChatOpenAI(
        #    model="gpt-3.5-turbo",
        #    temperature=0.1,
        #    max_tokens=2048,
        #    callbacks=[],
        #    verbose=False
        #)
        llm = ChatOpenAI(
            openai_api_key="sk-69560487bc6049d88a3461f96bae2891",
            openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
            model="qwen-plus",
            temperature=0.1,
            max_tokens=2048
        )
        logging.info(f"Done.")

        logging.info("Initialize tools...")
        embedding_model = eu.init_embedding_model()
        vector_store = eu.init_vector_store(embedding_model)
        logging.info(f"Done.")

        logging.info("Initialize chain...")
        chain_type_kwargs = {"prompt": QA_TEMPLATE_BAICHUAN, "verbose": verbose}
        self.conversation = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type='stuff',
            retriever=vector_store.as_retriever(search_kwargs={'k': 3}),
            chain_type_kwargs=chain_type_kwargs,
            return_source_documents=True,
            callbacks=[],
            verbose=False
        )
        logging.info(f"Done.")

        os.system("clear")
        streaming_print_banner()

    def ask(self, question):
            logging.info(f"Sending question to LLM: {question}") # æ·»åŠ è°ƒè¯•æ—¥å¿—
            try:
                # ç¡®ä¿ä¼ å…¥çš„æ˜¯æ­£ç¡®å®šä¹‰çš„å­—å…¸é”® 'query'
                result_dict = self.conversation.invoke({"query": question}) 
                result = result_dict['result']
                logging.info("LLM responded successfully.") # æ·»åŠ è°ƒè¯•æ—¥å¿—
                return result
            except Exception as e:
                logging.error(f"Error LLM: {e}")
                return "è¯·æ±‚å‡ºçŽ°é—®é¢˜ï¼ï¼ï¼"



    #def ask(self, question):
     #   try:
      #      result_dict = self.conversation.invoke({"query": question})
       #     result = result_dict['result']
        #    return result
        #except Exception as e:
         #   logging.error(f"Error LLM: {e}")
          #  return "è¯·æ±‚å‡ºçŽ°é—®é¢˜ï¼ï¼ï¼"


def main(args=None):
    IS_DUBUG = False
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    gpt = GPTAssistant(
        verbose=False,
    )
    if not IS_DUBUG:
        HOST = 'localhost'
        PORT = 5001
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((HOST, PORT))
        print("Connected to server.")

    while True:
        question = input(colors.YELLOW + "UserðŸ’¬> " + colors.ENDC)
        if question == "!quit" or question == "!exit":
            break
        if question == "!clear":
            os.system("clear")
            continue

        result = gpt.ask(question)  # Ask a question
        print(colors.GREEN + "AssistantðŸ¤–> " + colors.ENDC + f"{result}")
        
        if not IS_DUBUG:
            logging.info("Sending data to server...") # æ·»åŠ è°ƒè¯•æ—¥å¿—
            s.sendall(result.encode())
            logging.info("Data sent.")


if __name__ == '__main__':
    main()
